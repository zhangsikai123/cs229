{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS229: Problem Set 1\n",
    "## Problem 4: Linear Invariance of Optimization Algorithms\n",
    "\n",
    "**C. Combier**\n",
    "\n",
    "This iPython Notebook provides solutions to Stanford's CS229 (Machine Learning, Fall 2017) graduate course problem set 1, taught by Andrew Ng.\n",
    "\n",
    "The problem set can be found here: [./ps1.pdf](ps1.pdf)\n",
    "\n",
    "I chose to write the solutions to the coding questions in Python, whereas the Stanford class is taught with Matlab/Octave.\n",
    "\n",
    "## Notation\n",
    "\n",
    "- $x^i$ is the $i^{th}$ feature vector\n",
    "- $y^i$ is the expected outcome for the $i^{th}$ training example\n",
    "- $m$ is the number of training examples\n",
    "- $n$ is the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9J7p406abzgl"
   },
   "source": [
    "### Question 4.a)\n",
    "Let:\n",
    " - $z = A^{-1} x$\n",
    " - $g(z) = f(Az)$\n",
    " \n",
    "We also define the notation $H_f |_x$ and $\\nabla_f |_x$ , which are respectively the Hessian and Gradient of function $f$ evaluted at $x$.\n",
    " \n",
    "Write the update rule for Newton-Rhapson's method:\n",
    " \n",
    " $$\n",
    " \\begin{align*}\n",
    " z : &= z -H_g^{-1} |_z. \\nabla_g |_z \\\\\n",
    "  : &= z - H_f^{-1} |_{Az}. \\nabla_f |_{Az}\n",
    " \\end{align*}\n",
    " $$\n",
    " \n",
    "We calculate the Hessian using the chain rule:\n",
    " $$\n",
    " H_f|_{Az} = A^T . H_f |_{Az} . A \\implies H_f|_{Az} ^{-1}= A^{-1} . H_f|_{Az}^{-1} . A^{T^{-1}}\n",
    " $$\n",
    " \n",
    "Similarly, the chain rule applied to the gradient operator is given by:\n",
    " \n",
    " $$\n",
    " \\nabla_f |_{Az} = A^T . \\nabla_f|_{Az}\n",
    " $$\n",
    " \n",
    "Combining the two in the update rule:\n",
    " \n",
    "  $$\n",
    " \\begin{align*}\n",
    " z : &= z - A^{-1} . H_f|_{Az}^{-1}. A^{T^{-1}}. A^T  .\\nabla_f|_{Az} \\\\\n",
    " : &= z - A^{-1} H_f|_{Az}^{-1} .\\nabla_f|_{Az} \\\\\n",
    " : &= z - A^{-1} H_f|_x^{-1}  .\\nabla_f|_x \\\\\n",
    " : &= A^{-1} . (x -  H_f|_x^{-1} . \\nabla_f|_x )\n",
    " \\end{align*}\n",
    " $$\n",
    " \n",
    "Which proves the linearity of the Newton-Rhapson method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLXoC8jH6mlv"
   },
   "source": [
    "### Question 4.b)\n",
    "\n",
    "In this question, we show that gradient descent is not invariant to linear reparametrization.\n",
    "\n",
    "Consider the function $f:x \\mapsto x^2$, $x \\in R$\n",
    "\n",
    "We now consider the gradient descent update rule for this function and parameter $z = \\lambda x$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z:&= z - \\alpha \\frac{df}{dz} \\\\\n",
    ": &= z - \\alpha \\frac{df}{dx} \\frac{dx}{dz} \\\\\n",
    ": &= \\lambda x -\\frac{\\alpha}{\\lambda} (2 \\lambda x) = \\lambda x - \\alpha (2x) \\\\\n",
    "\\neq \\lambda.(x - \\alpha.\\frac{df}{dx}) = \\lambda (x - \\alpha.(2x))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This counter example shows that gradient descent is not invariant to linear reparametrization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
