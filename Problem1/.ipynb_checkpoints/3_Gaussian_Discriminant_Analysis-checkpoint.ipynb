{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS229: Problem Set 1\n",
    "## Problem 3: Gaussian Discriminant Analysis\n",
    "\n",
    "\n",
    "**C. Combier**\n",
    "\n",
    "This iPython Notebook provides solutions to Stanford's CS229 (Machine Learning, Fall 2017) graduate course problem set 1, taught by Andrew Ng.\n",
    "\n",
    "The problem set can be found here: [./ps1.pdf](ps1.pdf)\n",
    "\n",
    "I chose to write the solutions to the coding questions in Python, whereas the Stanford class is taught with Matlab/Octave.\n",
    "\n",
    "## Notation\n",
    "\n",
    "- $x^i$ is the $i^{th}$ feature vector\n",
    "- $y^i$ is the expected outcome for the $i^{th}$ training example\n",
    "- $m$ is the number of training examples\n",
    "- $n$ is the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.a)\n",
    "\n",
    "The gist of the solution is simply to apply Bayes rule, and simplify the exponential terms in the denominator which gives us the sigmoid function. The calculations are somewhat heavy:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(y=1 \\mid x) & = \\frac{p(x \\mid y=1)p(y=1)}{p(x)} \\\\\n",
    "              & = \\frac{p(x \\mid y=1)p(y=1)}{p(x \\mid y=1)p(y=1)+ p(x \\mid y=-1)p(y=-1)} \\\\\n",
    "              & = \\frac{\\frac{1}{(2\\pi)^{\\frac{n}{2}} \\lvert \\Sigma \\rvert^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2} \\left(x-\\mu_{1} \\right)^T\\Sigma^{-1} \\left(x-\\mu_{1} \\right) \\right) \\phi  }{ \\frac{1}{(2\\pi)^{\\frac{n}{2}} \\lvert \\Sigma \\rvert^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2} \\left(x-\\mu_{1} \\right)^T\\Sigma^{-1} \\left(x-\\mu_{1} \\right) \\right) \\phi + \\frac{1}{(2\\pi)^{\\frac{n}{2}} \\lvert \\Sigma \\rvert^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2} \\left(x-\\mu_{-1} \\right)^T\\Sigma^{-1} \\left(x-\\mu_{-1} \\right) \\right)\\left(1-\\phi \\right)} \\\\\n",
    "              & = \\frac{\\phi \\exp \\left(-\\frac{1}{2} \\left(x-\\mu_{1} \\right)^T\\Sigma^{-1} \\left(x-\\mu_{1} \\right) \\right) }{\\phi \\exp \\left(-\\frac{1}{2} \\left(x-\\mu_{1} \\right)^T\\Sigma^{-1} \\left(x-\\mu_{1} \\right) \\right)  +  \\left(1-\\phi \\right) \\exp \\left(-\\frac{1}{2} \\left(x-\\mu_{-1} \\right)^T\\Sigma^{-1} \\left(x-\\mu_{-1} \\right) \\right)} \\\\\n",
    "              & = \\frac{1}{1+ \\exp \\left(\\log\\left(\\frac{\\left(1-\\phi \\right)}{\\phi}\\right) -\\frac{1}{2} \\left(x-\\mu_{-1} \\right)^T\\Sigma^{-1} \\left(x-\\mu_{-1} \\right) + \\frac{1}{2} \\left(x-\\mu_{1} \\right)^T\\Sigma^{-1} \\left(x-\\mu_{1} \\right) \\right)} \\\\\n",
    "              & = \\frac{1}{1+\\exp \\left(\\log \\left(\\frac{1-\\phi}{\\phi}\\right) -\\frac{1}{2} \\left(x^T \\Sigma^{-1}x -2x^T \\Sigma^{-1}\\mu_{-1}+ \\mu_{-1}^T \\Sigma^{-1} \\mu_{-1}\\right) + \\frac{1}{2} \\left(x^T \\Sigma^{-1}x -2x^T \\Sigma^{-1}\\mu_{1}+ \\mu_{1}^T \\Sigma^{-1} \\mu_{1} \\right)\\right)} \\\\\n",
    "              & = \\frac{1}{1+\\exp \\left(\\log \\left(\\frac{1-\\phi}{\\phi}\\right) + x^T \\Sigma^{-1} \\mu_{-1} - x^T \\Sigma^{-1} \\mu_1 - \\frac{1}{2} \\mu_{-1}^T \\Sigma^{-1} \\mu_{-1} + \\frac{1}{2} \\mu_1^T\\Sigma^{-1}\\mu_1 \\right)} \\\\\n",
    "              & = \\frac{1}{1+ \\exp\\left(\\log\\left(\\frac{1-\\phi}{\\phi}\\right) + x^T \\Sigma^{-1} \\left(\\mu_{-1}-\\mu_1 \\right) - \\frac{1}{2}\\mu_{-1}^T\\Sigma^{-1}\\mu_{-1} + \\mu_1^T \\Sigma^{-1} \\mu_1 \\right)} \\\\\n",
    "              \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "With:\n",
    "- $\\theta_0 = \\frac{1}{2}\\left(\\mu_{-1}^T \\Sigma^{-1} \\mu_{-1}- \\mu_1^T \\Sigma^{-1}\\mu_1 \\right)-\\log\\frac{1-\\phi}{\\phi} $\n",
    "- $\\theta = \\Sigma^{-1}\\left(\\mu_{1}-\\mu_{-1} \\right)$\n",
    "\n",
    "we have:\n",
    "\n",
    "$$\n",
    "p(y=1 \\mid x) = \\frac{1}{1+\\exp \\left(-y(\\theta^Tx + \\theta_0) \\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions 3.b) and 3.c)\n",
    "Question 3.b) is the special case where $n=1$. Let us prove the general case directly, as required in 3.c):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\ell \\left(\\phi, \\mu_{-1}, \\mu_1, \\Sigma \\right)  & = \\log \\prod_{i=1}^m p(x^{i}\\mid y^i; \\phi, \\mu_{-1}, \\mu_1, \\Sigma)p(y^{i};\\phi) \\\\\n",
    "                                                  & = \\sum_{i=1}^m \\log p(x^{i}\\mid y^{i}; \\phi, \\mu_{-1}, \\mu_1, \\Sigma) + \\sum_{i=1}^m \\log p(y^{i};\\phi) \\\\\n",
    "                                                  & = \\sum_{i=1}^m \\left[\\log \\frac{1}{\\left(2 \\pi \\right)^{\\frac{n}{2}} \\lvert \\Sigma \\rvert^{\\frac{1}{2}}} - \\frac{1}{2} \\left(x^{i} - \\mu_{y^{i}}  \\right)^T \\Sigma^{-1} \\left(x^{i} - \\mu_{y^{i}}  \\right) + \\log \\phi^{y^{i}} + \\log \\left(1- \\phi \\right)^{\\left(1-y^{i} \\right)} \\right] \\\\\n",
    "                                                  & \\simeq \\sum_{i=1}^m \\left[- \\frac{1}{2} \\log \\lvert \\Sigma \\rvert - \\frac{1}{2} \\left(x^{i} - \\mu_{y^{i}}  \\right)^T \\Sigma^{-1} \\left(x^{i} - \\mu_{y^{i}}  \\right) + y^{i} \\log \\phi + \\left(1-y^{i} \\right) \\log \\left(1- \\phi \\right) \\right] \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now we calculate the maximum likelihood be calculating the gradient of the log-likelihood with respect to the parameters and setting it to $0$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ell}{\\partial \\phi} &= \\sum_{i=1}^{m}( \\frac{y^i}{\\phi} - \\frac{1-y^i}{1-\\phi}) \\\\\n",
    "&= \\sum_{i=1}^{m}\\frac{1(y^i = 1)}{\\phi} + \\frac{m-\\sum_{i=1}^{m}1(y^i = 1)}{1-\\phi}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore, $\\phi = \\frac{1}{m} \\sum_{i=1}^m 1(y^i =1 )$, i.e. the percentage of the training examples such that $y^i = 1$\n",
    "\n",
    "Now for $\\mu_{-1}:$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\mu_{-1}} \\ell  & = - \\frac{1}{2} \\sum_{i : y^{i}=-1} \\nabla_{\\mu_{-1}} \\left[ -2 \\mu_{-1}^T \\Sigma^{-1} x^{(i)} + \\mu_{-1}^T \\Sigma^{-1} \\mu_{-1}  \\right] \\\\\n",
    "                                                                    & = - \\frac{1}{2} \\sum_{i : y^{i}=-1} \\left[-2 \\Sigma^{-1}x^{(i)} + 2  \\Sigma^{-1} \\mu_{-1} \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Again, we set the gradient to $0$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "                                                                  \\sum_{i:y^i=-1} \\left[\\Sigma^{-1}x^{i}-\\Sigma^{-1} \\mu_{-1} \\right] &= 0 \\\\\n",
    "                                                                  \\sum_{i=1}^m 1 \\left\\{y^{i}=-1\\right\\} \\Sigma^{-1} x^{(i)} - \\sum_{i=1}^m 1 \\left\\{y^{i}=-1 \\right\\} \\Sigma^{-1} \\mu_{-1} &=0 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "This yields:\n",
    "$$\n",
    "\\Sigma^{-1} \\mu_{-1} \\sum_{i=1}^m 1 \\left\\{y^{i}=-1 \\right\\} = \\Sigma^{-1} \\sum_{i=1}^m 1 \\left\\{y^{(i)}=-1\\right\\}  x^{i}\n",
    "$$\n",
    "Allowing us to finally write:\n",
    "$$\\mu_{-1} = \\frac{\\sum_{i=1}^m 1 \\left\\{y^{i}=-1\\right\\}  x^{i}}{\\sum_{i=1}^m 1 \\left\\{y^{(i)}=-1 \\right\\}}$$\n",
    "\n",
    "The calculations are similar for $\\mu_1$, and we obtain:\n",
    "$$\\mu_{1} = \\frac{\\sum_{i=1}^m 1 \\left\\{y^{i}=1\\right\\}  x^{i}}{\\sum_{i=1}^m 1 \\left\\{y^{i}=1 \\right\\}}$$\n",
    "\n",
    "\n",
    "The last step is to calculate the gradient with respect to $\\Sigma$. To simplify calculations, let us calculate the gradient for $S = \\frac{1}{\\Sigma}$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{S} \\ell  & = - \\frac{1}{2}\\sum_{i=1}^m \\nabla_{\\Sigma} \\left[-\\log \\lvert S \\rvert + \\left(x^{i}- \\mu_{y^{i}} \\right)^T S \\left(x^{i}- \\mu_{y^{i}} \\right) \\right] \\\\\n",
    "                                                                  & = - \\frac{1}{2}\\sum_{i=1}^m \\left[-S^{-1} + \\left(x^{i}- \\mu_{y^{i}} \\right)\\left(x^{i}- \\mu_{y^{i}} \\right)^T \\right] \\\\\n",
    "                                                                  & = \\sum_{i=1}^m \\frac{1}{2} \\Sigma - \\frac{1}{2} \\sum_{i=1}^m \\left(x^{i}- \\mu_{y^{i}} \\right)\\left(x^{i}- \\mu_{y^{i}} \\right)^T\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Again, we set the gradient to $0$, allowing us to write:\n",
    "$$\n",
    "\\frac{1}{2} m \\Sigma = \\frac{1}{2} \\sum_{i=1}^m \\left(x^{i}- \\mu_{y^{i}} \\right)\\left(x^{i}- \\mu_{y^{i}} \\right)^T \\\\\n",
    "$$\n",
    "Finally, we obtain the maximum likelihood estimate for $\\Sigma$:\n",
    "$$\n",
    "\\Sigma = \\frac{1}{m}\\sum_{i=1}^m \\left(x^{i}- \\mu_{y^{i}} \\right)\\left(x^{i}- \\mu_{y^{i}} \\right)^T\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
