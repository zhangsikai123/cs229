{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS229: Problem Set 2\n",
    "## Problem 2: Model Calibration\n",
    "\n",
    "\n",
    "**C. Combier**\n",
    "\n",
    "This iPython Notebook provides solutions to Stanford's CS229 (Machine Learning, Fall 2017) graduate course problem set 2, taught by Andrew Ng.\n",
    "\n",
    "The problem set can be found here: [./ps2.pdf](ps2.pdf)\n",
    "\n",
    "I chose to write the solutions to the coding questions in Python, whereas the Stanford class is taught with Matlab/Octave.\n",
    "\n",
    "## Notation\n",
    "\n",
    "- $x^i$ is the $i^{th}$ feature vector\n",
    "- $y^i$ is the expected outcome for the $i^{th}$ training example\n",
    "- $m$ is the number of training examples\n",
    "- $n$ is the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rsUtreJMonLw"
   },
   "source": [
    "### Question 2.a)\n",
    "\n",
    "The maximum likelihood parameters $\\theta^*$ are obtained by writing the gradient of the log-likelihood with respect to $\\theta$ and setting it to $0$. In matrix form, this is equivalent to solving the following equation:\n",
    "\n",
    "$$\n",
    "X^T(Y-h_{\\theta}(X)) = 0\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $X$ is an $m \\times (n+1)$ matrix, given the addition of the intercept term $x_0 = 1 \\hspace{1em}, \\forall i$\n",
    "- $Y$ is an $m \\times 1$ matrix\n",
    "\n",
    "Expanding the matrix equation for $\\theta = \\theta^*$ gives:\n",
    "\n",
    "$$\n",
    "  \\left[ {\\begin{array}{cccc}\n",
    "   1 & ... & 1 \\\\\n",
    "   x^1_1 & ... & x_n^1 \\\\\n",
    "    & ... & \\\\\n",
    "    x^m_1 & ... & x^m_n\n",
    "  \\end{array} } \\right]\n",
    "  (Y-h_{\\theta^*}(X)) = 0\n",
    "  $$\n",
    "  \n",
    "  If we extract the first line from the above matrix equation, we get:\n",
    "  \n",
    "  $$\n",
    "  \\sum_{i=1}^m y^i = \\sum_{i=1}^m h_{\\theta^*}(x^i)\n",
    "  $$\n",
    "  \n",
    "  Using the definition of $h_{\\theta^*}$:\n",
    "  \n",
    "  $$\n",
    "  \\sum_{i=1}^m 1(y^i = 1) = \\sum_{i=1}^m P(y = 1|x;\\theta^*)\n",
    "  $$\n",
    "  \n",
    "  We conclule by saying that $|\\{ i \\in I_{0,1} \\}| = m$ which shows the property holds true for $(a,b) = (0,1)$\n",
    "  \n",
    "  ### Question 2.b)\n",
    "  \n",
    "  - If a model is perfectly callibrated, then all we can say is that the probabilities output from the model match empirical observations. This only describes the probabilities of the outcomes, and not the outcomes themselves, therefore the model does not necessarily achieve perfect accuracy.\n",
    "  - Conversely, if a model has perfect accuracy, then the probabilities output by the model necessarily match empirical observations\n",
    "  \n",
    "  \n",
    "  This implies that callibration is a weaker assumption than accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
